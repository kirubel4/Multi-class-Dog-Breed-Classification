{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy7itcm7n2Mc"
      },
      "source": [
        "## üê∂End-to-end Multi-class Dog Breed Classification using colab\n",
        "\n",
        "This notebook builds an end-to-end multi-class image classifier using TensorFlow and TensorFlow Hub.\n",
        "\n",
        "## 1.Problem\n",
        "\n",
        "Identifying the breed of a dog given an image of a dog.\n",
        "\n",
        "When I'm sitting at the cafe and I take a photo of a dog, want to know what bredd of dof it is.\n",
        "\n",
        "## 2.Data\n",
        "\n",
        "The data we're using is from Kaggle's dog breed identication competition.\n",
        "https://www.Kaggle.com/c/dog-breed-identification/data\n",
        "\n",
        "## 3.Evaluation\n",
        "\n",
        "The evalutation is a file with prediction probabilites for each dog breed of each test image.\n",
        "https://www.kaggle.com/c/dog-breed-identification/overview/evaluation\n",
        "\n",
        "## 4.Features\n",
        "\n",
        "Some information about the data:\n",
        "* we're deakubg with `image`(unstructured data) so it's probably best we use deep learning/transfer learning.\n",
        "* There are 120 breeds of dogs(this means there are 120 different classes).\n",
        "* There are around 10,000+ images in the training set(these images have labels).\n",
        "* There are around 10,000+ images in the test set(these have no labels,because we'll want to predict them).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL1BfV2fSMLN"
      },
      "source": [
        "## Get our workspace ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjhE72hDSTEA"
      },
      "outputs": [],
      "source": [
        "# Import TensorFlow into Colab\n",
        "import tensorflow as tf\n",
        "print(\"TF version\", tf.__version__)\n",
        "import tensorflow_hub as hub\n",
        "print(\"TF Hub version\", hub.__version__)\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"GPU\", \"available (YESSSSSSSSSSS!!!!)\" if tf.config.list_physical_devices(\"GPU\") else \"not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhui7FaL_uBu"
      },
      "source": [
        " ## Getting o ur data ready (turning into Tensors)\n",
        " with all machine learning models, our data has to be in numerical format. So that's what i will be doing first. Truning our images into TEnsors(numerical representation).\n",
        " Let's start by accessing our data and checking out the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNABi7Gll9VI"
      },
      "outputs": [],
      "source": [
        "# checkout the labels of our data\n",
        "import pandas as pd\n",
        "labels_csv = pd.read_csv(\"drive/MyDrive/dog-vision/dog-breed-identification.zip (Unzipped Files)/labels.csv\")\n",
        "print(labels_csv.describe())\n",
        "print(labels_csv.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksf8xonvoee1"
      },
      "outputs": [],
      "source": [
        "labels_csv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xjqlfh3oiAj"
      },
      "outputs": [],
      "source": [
        "# how many images are there of each breed?\n",
        "labels_csv[\"breed\"].value_counts().plot.bar(figsize=(20,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q51r9LLpbvc"
      },
      "outputs": [],
      "source": [
        "labels_csv[\"breed\"].value_counts().median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joukMOX6sPG1"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(\"drive/MyDrive/dog-vision/dog-breed-identification/train/001513dfcb2ffafc82cccf4d8bbaba97.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPclLIxnbqBX"
      },
      "source": [
        "### Getting images and their labels\n",
        "getting a list of all of the images file pathnames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vkPdWUGgWmZ"
      },
      "outputs": [],
      "source": [
        "# Create pathnames form image ID\"s\n",
        "filename = [\"drive/MyDrive/dog-vision/dog-breed-identification/train/\" + fname + \".jpg\" for fname in labels_csv[\"id\"]]\n",
        "filename[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efVMMzU5hImf"
      },
      "outputs": [],
      "source": [
        "# Check whether number of filenames matches number of actual images file\n",
        "\n",
        "import os\n",
        "if len(os.listdir(\"drive/MyDrive/dog-vision/dog-breed-identification/train/\")) == len(filename):\n",
        "  print(\"Filename match actual amout of files!!! proceed\")\n",
        "else:\n",
        "  print(\"Filename do not mach with actual amount of files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fGMhJVRj594"
      },
      "outputs": [],
      "source": [
        "#one more check\n",
        "Image(filename[9000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIwe0BrpkRQJ"
      },
      "outputs": [],
      "source": [
        "labels_csv['breed'][9000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QVeDb7WkxXG"
      },
      "source": [
        "Since i now got my training images filepath in a list, prepare my labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_HC34mhyO8A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "labels = labels_csv[\"breed\"].to_numpy()\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qAucDh9yzAr"
      },
      "outputs": [],
      "source": [
        "# see if number of lables matches the number of filenames\n",
        "if len(labels) == len(filename):\n",
        "  print(\"Number matches\")\n",
        "else:\n",
        "  print(\"Number don't matches\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzqQLQOrzpK1"
      },
      "outputs": [],
      "source": [
        "# find the unique label values\n",
        "unique_breeds= np.unique(labels)\n",
        "len(unique_breeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbJbGeC92Z4N"
      },
      "outputs": [],
      "source": [
        "# Trun a single labels into an array of  booleans\n",
        "print(labels[0])\n",
        "labels[0] == unique_breeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TTdEIG224W_"
      },
      "outputs": [],
      "source": [
        "# Turn every labels into a booleans array\n",
        "boolean_labels = [label == unique_breeds for label in labels]\n",
        "boolean_labels[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t04RzbAn_9Gz"
      },
      "outputs": [],
      "source": [
        "# Example: turning boolean array into integers\n",
        "print(labels[0])\n",
        "print(np.where(unique_breeds == labels[0]))# index where label occurs\n",
        "print(boolean_labels[0].argmax())# index where labels occures in boolean array\n",
        "print(boolean_labels[0].astype(int))# there will be a 1 where the sample label occurs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gk8UkkRDcZq"
      },
      "source": [
        "### Creating our own validation set\n",
        "since the dataset from kaggle doesn't come with a validation set, i am going to create my own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D72lyOnYCN5i"
      },
      "outputs": [],
      "source": [
        "# Setup x and y variable\n",
        "x = filename\n",
        "y = boolean_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2v7-ZG6FXS1"
      },
      "source": [
        "I am going to start off experimenting with ~1000 images and increase as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKPFOPRhFRHo"
      },
      "outputs": [],
      "source": [
        "# set number of images to use for experimenting\n",
        "NUM_IMAGES = 1000 #@param {type:\"slider\", min:1000, max:10000, step:1000}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc5Tnx15aRsz"
      },
      "outputs": [],
      "source": [
        "# spliting data into train and validation set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x[:NUM_IMAGES],\n",
        "                                                  y[:NUM_IMAGES],\n",
        "                                                  test_size=0.2,\n",
        "                                                  random_state=42)\n",
        "len(x_train), len(y_train), len(x_val), len(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-0MPi_Pbcrw"
      },
      "outputs": [],
      "source": [
        "# Lets have a geez the training data\n",
        "x_train[:5], y_train[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0-LM4JFcgkp"
      },
      "source": [
        "## Preprocessing images(tuning images into Tensors)\n",
        "To preprocess our images into Tensors we're going to write a function which does a few things:\n",
        "1. Take an image filepath as input\n",
        "2. Use TensorFlow to read the file and save it to a variable, `image`\n",
        "3. Turn our `image` (a jpg) into Tensors\n",
        "4. Normalize our image(convert color channel values form 0-225 to 0-1).\n",
        "5. Resize the `image` to be a shape of (224,224)\n",
        "6. Return the modified `image`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKaCUeoxb71H"
      },
      "outputs": [],
      "source": [
        "# convert images to numpy array\n",
        "from matplotlib.pyplot import imread\n",
        "image = imread(filename[42])\n",
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKtDIedDkZ1i"
      },
      "outputs": [],
      "source": [
        "tf.constant(image[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCNVjcNLyJe5"
      },
      "outputs": [],
      "source": [
        "# Define image size\n",
        "IMG_SIZE = 224\n",
        "\n",
        "#Create a function for preprocessing images\n",
        "def process_image(image_path, img_size= IMG_SIZE):\n",
        "  \"\"\"\n",
        "  Takes an image file path and turns hte image into a Tensor.\n",
        "  \"\"\"\n",
        "  #Read in an image file\n",
        "  image = tf.io.read_file(image_path)\n",
        "  #Turn the jpeh image into numerical Tensor with 3 colour channels (Red, Green,Blue)\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  # conver the colour channel values to from 0-255 to 0-1 values\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  #Resize the image to desired value (224, 224)\n",
        "  image = tf.image.resize(image, size=[IMG_SIZE,IMG_SIZE])\n",
        "\n",
        "  return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Zw3AszS3Q-"
      },
      "source": [
        "## Turning our data into batches\n",
        "Why turn our data into batches?\n",
        "\n",
        "Let's say you're trying to process 10,000+ images in one go... they all might not fit into memory.\n",
        "\n",
        "So that's why we do about 32 (this is the batch size) images at a time (you can manually adjust the batch size if need be).\n",
        "\n",
        "In order to use TensorFlow effectively, we need our data in the form of Tensor tuples which look like this:\n",
        "`(image, label)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsxZKCtpyWzf"
      },
      "outputs": [],
      "source": [
        "# Create a simple funtion to return a tuple (image, label)\n",
        "\n",
        "def get_image_label(image_path, label):\n",
        "  \"\"\"\n",
        "  Takes an image file path name and the assosciated label,\n",
        "  processes the image and return a tuple of (image, label).\n",
        "  \"\"\"\n",
        "  image = process_image(image_path)\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5hDBXde30f_"
      },
      "outputs": [],
      "source": [
        "(process_image(x[42], tf.constant(y[42])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyUc6Qx955IM"
      },
      "source": [
        "Now we've got a way to turn our data into tuples of Tensors in the form:`(image, label)` let's make a function to turn all of our data (`x`& `y`) into batches!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rtvob5O15Ebe"
      },
      "outputs": [],
      "source": [
        "# Define the batch size, 32 is a good start\n",
        "BATCH_SIZE = 32\n",
        "def create_data_batches(x, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):\n",
        "  \"\"\"\n",
        "    creates batches of data out of image (x) and label (y) pairs.\n",
        "    shuffles the data if it's training data but doesn't shuffle if it's validation data.\n",
        "    also accepts test data as input(no labels).\n",
        "  \"\"\"\n",
        "  #if the data is a test dataset, we probably don't have have labels\n",
        "  if test_data:\n",
        "    print(\"Creating test data batches....\")\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(x))) #only filepath (No labels)\n",
        "    data_batch = data.map(process_image).batch(BATCH_SIZE)\n",
        "    return data_batch\n",
        "\n",
        "  # if the data is a valid dataset, dwe don't need to shuffle it\n",
        "  elif valid_data:\n",
        "    print(\"Creating validation data batches....\")\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(x),# filepaths\n",
        "                                               tf.constant(y)))#labels\n",
        "    data_batch = data.map(get_image_label).batch(BATCH_SIZE) # Use get_image_label to get both image and label\n",
        "    return data_batch\n",
        "\n",
        "  else:\n",
        "    print(\"Creating training data batches....\")\n",
        "    #Trun filepaths and labels into Tensors\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(x),\n",
        "                                               tf.constant(y)))\n",
        "    # shuffling pathnames and labels before mapping image processor function is faster than shuffling images\n",
        "    data = data.shuffle(buffer_size=len(x))\n",
        "    # create (image, label) tuples (this also turns the image path into a preprocessed image)\n",
        "    data = data.map(get_image_label)\n",
        "    #turn the training data into baches\n",
        "    data_batch = data.batch(BATCH_SIZE)\n",
        "  return data_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8M6kXOXPg6a"
      },
      "outputs": [],
      "source": [
        "# Create training and validation data batches\n",
        "train_data = create_data_batches(x_train, y_train)\n",
        "val_data = create_data_batches(x_val, y_val, valid_data=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8RRxaHNPpH3"
      },
      "outputs": [],
      "source": [
        "# Check out the differenct attribute of our data batches\n",
        "train_data.element_spec, val_data.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcGHxA9NQOuY"
      },
      "source": [
        "## Visualizing data batches\n",
        "\n",
        "our data is now in batches however these can be a little hard ot understand/comperhand, let's zisualize them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Milrvfd7ijgD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a fuction for viewing images in a data batch\n",
        "def show_25_images(image, labels):\n",
        "  \"\"\"\n",
        "  Displays 25 images from a data batch.\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for i in range(25):\n",
        "    # create subplots (5 rows, 5 columns)\n",
        "    ax = plt.subplot(5,5, i+1)\n",
        "    plt.imshow(image[i])\n",
        "    plt.title(unique_breeds[labels[i].argmax()])\n",
        "    #turn the grid line off\n",
        "    plt.axis(\"off\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDiXbaYEnLtf"
      },
      "outputs": [],
      "source": [
        "# Now let's visualize the data in a training batch\n",
        "train_images, train_labels = next(train_data.as_numpy_iterator())\n",
        "show_25_images(train_images, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmE2jMQGBdYh"
      },
      "outputs": [],
      "source": [
        "val_images, val_labels = next(val_data.as_numpy_iterator())\n",
        "show_25_images(val_images, val_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6j3TpZfphZa"
      },
      "source": [
        "## building a model\n",
        "\n",
        "Before we build a model, there are a few things we need to define:\n",
        "* The  input shape (our images shape, in the form of Tensore) to our model.\n",
        "* The output shape (images labels, in the form of Tensors) of our model.\n",
        "* The URL of the model we want to use from TensorFlow Hub-https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB7doIpS6nE9"
      },
      "source": [
        "NOw we've got our inputs, outputs and model ready to go.\n",
        "Let's put htem together into a keras deep learning model!\n",
        "\n",
        "Knowing this , let's create a function which:\n",
        "* Takes  the input shape, output shape and the modle we've chosen as parameters.\n",
        "* Define the layers in  keras model  in seuential fashion(do this first, then this, then that)\n",
        "* Compiles the model(says it should be evaluated and improved)\n",
        "* Builds the model (tells the model the input shape it'll be getting)\n",
        "* Returns the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmlhN9AFNFRl"
      },
      "outputs": [],
      "source": [
        "#build a function to train and return a trained model\n",
        "def train_model():\n",
        "    \"\"\"\n",
        "    Train a given model and retrun the trained version.\n",
        "    \"\"\"\n",
        "    # Create a model\n",
        "    model = create_model()\n",
        "\n",
        "    #create new TesnorBoard session everytime we train a model\n",
        "    tensorboard = create_tensorboard_callback()\n",
        "    # Fit the model to the data passing it the call backs we created\n",
        "    model.fit(x=train_data,\n",
        "              epochs=NUM_EPOCHS,\n",
        "              validation_data=val_data,\n",
        "              validation_freq=1,\n",
        "              callbacks=[tensorboard, early_stopping])\n",
        "    # return the fitted model\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOglWoyVNGne"
      },
      "source": [
        "## Createing callbacks\n",
        "\n",
        "callbacks are helper function a modle can use during training to do such things as save its progress,check its progress or stop training early if a model stop improving.\n",
        "\n",
        "we'll create two callbacks, one for TensorBoard which helps track our models progress and another for early stopping which prevents out model from training for too long.\n",
        "## TensorBoard Callback\n",
        "\n",
        "To setup a TesnorBoard callback, we need to do 3 things:\n",
        "1. Load the TensorBoard notebook extension.\n",
        "2. create a TensorBoard callback which is able to save logs to a directory and pass it to our model's `fit()` function.\n",
        "3. Visualize our models training logs with the `%tensorboard` magic function(we'll do this after model training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOFztMtXKea5"
      },
      "outputs": [],
      "source": [
        "# Load TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "561ilMjPNUmc"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "# crate a function to build  a Tenosrboard callback\n",
        "def create_tensorboard_callback():\n",
        "  #create a log directory to track logs when ever it runs\n",
        "  logdir = os.path.join(\"drive/MyDrive/dog-vision/dog-breed-identification/logs\",\n",
        "                        datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  return tf.keras.callbacks.TensorBoard(logdir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5ICt8bsQH_q"
      },
      "source": [
        "## Early Stopping callback\n",
        "Early stopping helps stop our model from overfitting by stopping training if a certain evaluration metric stops improving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ-EPWHDaVrZ"
      },
      "outputs": [],
      "source": [
        "# create early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\",\n",
        "                                                  patience=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aApRAQvarOk"
      },
      "source": [
        "## Training a model (on subset of data)\n",
        "\n",
        "Our fist model is only going to train on 1000 images, to make sure everything is working."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eckMhEx-cpvL"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 100 #@param {type:\"slider\", min:10, max:100, step:10}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTi4GFbjdDit"
      },
      "outputs": [],
      "source": [
        "# Check to make sure we're still running on a GPU\n",
        "print(\"GPU\", \"available (YESSSSSSSSSSS!!!!)\" if tf.config.list_physical_devices(\"GPU\") else \"not available :(\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36d3p56EfVf-"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = (224, 224, 3)\n",
        "OUTPUT_SHAPE = len(unique_breeds)\n",
        "\n",
        "def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE):\n",
        "    print(\"Building model with tf.keras.applications.EfficientNetB0\")\n",
        "\n",
        "    input_layer = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # Use built-in EfficientNetB0 (pre-trained on ImageNet, feature extractor mode)\n",
        "    base_model = tf.keras.applications.EfficientNetB0(\n",
        "        include_top=False,  # Remove the top classification layers\n",
        "        weights='imagenet',\n",
        "        input_shape=input_shape,\n",
        "        pooling='avg'  # Global average pooling to get a feature vector\n",
        "    )(input_layer)\n",
        "    base_model.trainable = False  # Freeze the base\n",
        "\n",
        "    # Add output layer\n",
        "    output_layer = tf.keras.layers.Dense(units=output_shape, activation=\"softmax\")(base_model)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-YFuLwzkxUG"
      },
      "source": [
        " the model is overfitting which is good thing to start with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S21uVbLla7EP"
      },
      "outputs": [],
      "source": [
        "model = train_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dTIKcpwhmIo"
      },
      "source": [
        "### Checking the TesnorBoard logs\n",
        "\n",
        "The TesnorBoard magic function(%tensorboard) will access he logs directory we created earlier and visualize its contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WUB59zUieQF"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir drive/MyDrive/dog-vision/dog-breed-identification/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k23KHaPHjNEM"
      },
      "source": [
        "## Making and evaluating prediction using a trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBQbvd9Il7cX"
      },
      "outputs": [],
      "source": [
        "# Make prediction on the validation data (not used to train on)\n",
        "predictions =  model.predict(val_data, verbose=1)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG7t2xcAmW3X"
      },
      "outputs": [],
      "source": [
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw-dyoAJ3UBF"
      },
      "outputs": [],
      "source": [
        "predictions[81]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmho6qvR3brf"
      },
      "outputs": [],
      "source": [
        "unique_breeds[np.argmax(predictions[20])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqKD31MYniRk"
      },
      "outputs": [],
      "source": [
        "predictions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd4K_SlVmiq1"
      },
      "outputs": [],
      "source": [
        "np.sum(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23r4holDmz2t"
      },
      "outputs": [],
      "source": [
        "# First prediction\n",
        "index = 97\n",
        "print(predictions[index])\n",
        "print(f\"Max value (probability of predictioni): {np.max(predictions[index])}\")\n",
        "print(f\"Sum: {np.sum(predictions[index])}\")\n",
        "print(f\"Max index: {np.argmax(predictions[index])}\")\n",
        "print(f\"predicted label: {unique_breeds[np.argmax(predictions[index])]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyjwYRofpjER"
      },
      "outputs": [],
      "source": [
        "unique_breeds[63]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvn3i_Vdqc3d"
      },
      "source": [
        "Having the above funtionality is greate but we want to be able to do it at scale.\n",
        "\n",
        "And it would be even better if we could see the image the prediction is being made on!\n",
        "\n",
        "**Note**: prediction probabilities are also known as confidence levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njhANwABuSOJ"
      },
      "outputs": [],
      "source": [
        "#Trun prediction probablity into their respective label (easier to understand)\n",
        "def get_pred_label(predictions_probabilities):\n",
        "  \"\"\"\n",
        "  Turns an array of prediction probabilities into a label.\n",
        "  \"\"\"\n",
        "  return unique_breeds[np.argmax(predictions_probabilities)]\n",
        "# Get a predicted label based on an array of predictions probabilities\n",
        "pred_label = get_pred_label(predictions[90])\n",
        "pred_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsldG4J20APR"
      },
      "outputs": [],
      "source": [
        "unique_breeds[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvWYy5Rd5EOV"
      },
      "outputs": [],
      "source": [
        "val_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3LXz48ke6pj"
      },
      "source": [
        "Now since our validation data is still in a batch dataset,\n",
        "we'll have to unbatchify it to make prediction on the validation images and then compaer those prediction to the validation labels(true labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7wBmnt_fYfE"
      },
      "outputs": [],
      "source": [
        "# create a function to unbatch a batch dataset\n",
        "def unbatchify(data):\n",
        "  \"\"\"\n",
        "  Takes a batched dataset of (images, label) Tensor and retrun separate arrays of image and labels.\n",
        "  \"\"\"\n",
        "  images= []\n",
        "  labels= []\n",
        "  # loop through unbatched data\n",
        "  for image, label in data.unbatch().as_numpy_iterator():\n",
        "    images.append(image)\n",
        "    labels.append(unique_breeds[np.argmax(label)])\n",
        "  return images, labels\n",
        "\n",
        "# Unbatchify the validation data\n",
        "val_images, val_labels = unbatchify(val_data)\n",
        "val_images[0], val_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3nABXfHigEw"
      },
      "outputs": [],
      "source": [
        "get_pred_label(val_images[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rYJE8hhjlcH"
      },
      "source": [
        "Now we've got ways to get:\n",
        "  * prediction labels\n",
        "  * validation labels(train_data)\n",
        "  * validation images\n",
        "\n",
        "Let's make some functions to make these all a bit more visualize.\n",
        "\n",
        "we'll create a function which:\n",
        "* Take an array of predictions probabilities, an array of truth labels and un array of images and integers.\n",
        "* convert the prediction probabilities to predicted label.\n",
        "* Plot the predicted lable,it's predicted probability, the truth label and the target image on s single plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiVB_CBrxZYS"
      },
      "outputs": [],
      "source": [
        "def plot_pred(prediction_probabilities, labels, images, n=1):\n",
        "  \"\"\"\n",
        "  View the prediction, ground truth and image for sample n\n",
        "  \"\"\"\n",
        "  pred_prob, true_label, image = prediction_probabilities[n], labels[n],images[n]\n",
        "\n",
        "  #Get the pred label\n",
        "  pred_label = get_pred_label(pred_prob)\n",
        "\n",
        "  # Plot image and remove ticks\n",
        "  plt.imshow(image)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  #change plot title to predicted\n",
        "  if pred_label == true_label:\n",
        "    color = \"green\"\n",
        "  else:\n",
        "    color = \"red\"\n",
        "\n",
        "  # change plot titile to be predicted, probabiltiy of prediction and truth label\n",
        "  plt.title(\"{} {:2.0f}% {}\".format(pred_label,\n",
        "                                    np.max(pred_prob)*100,\n",
        "                                    true_label),\n",
        "                                    color = color\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVdT-m_4ztcN"
      },
      "outputs": [],
      "source": [
        "plot_pred(prediction_probabilities=predictions,\n",
        "          labels = val_labels,\n",
        "          images = val_images,\n",
        "          n=77)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYKYoOy6348r"
      },
      "source": [
        "Now we've got one function to visualize our modle top predictions, let's make another to view our model top 10 predictions.\n",
        "\n",
        "This function will:\n",
        "* Take an input of prediction probabilites array and an integer\n",
        "* Find the prediction using get_pred_label()\n",
        "* find the top 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycKwB75lLVXh"
      },
      "outputs": [],
      "source": [
        "def plot_pred_conf(prediction_probabilities,labels, n=1):\n",
        "  \"\"\"\n",
        "  Plot the top 10 highest prediction confidences along with the truth label for sample n.\n",
        "  \"\"\"\n",
        "  pred_prob, true_label = prediction_probabilities[n], val_labels[n]\n",
        "\n",
        "  # Get the predicted label\n",
        "  pred_label = get_pred_label(pred_prob)\n",
        "\n",
        "  # Find the top 10 prediction\n",
        "  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]\n",
        "  # find the top 10 prediction confidence values\n",
        "  top_10_pred_values = pred_prob[top_10_pred_indexes]\n",
        "  # Find the top 10 prediction labels\n",
        "  top_10_pred_labels = unique_breeds[top_10_pred_indexes]\n",
        "\n",
        "  # Setup plot\n",
        "  top_plot = plt.bar(np.arange(len(top_10_pred_labels)),\n",
        "                     top_10_pred_values,\n",
        "                     color = \"grey\")\n",
        "  plt.xticks(np.arange(len(top_10_pred_labels)),\n",
        "             labels=top_10_pred_labels,\n",
        "             rotation=\"vertical\")\n",
        "\n",
        "  # Change color of true label\n",
        "  if np.isin(true_label, top_10_pred_labels):\n",
        "    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color(\"green\")\n",
        "  else:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2vj73BNPyY-"
      },
      "outputs": [],
      "source": [
        "plot_pred_conf(prediction_probabilities=predictions,\n",
        "               labels=val_labels,\n",
        "               n= 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got some function to help us visualize our predictions and evaluate our model, let's check out.."
      ],
      "metadata": {
        "id": "iCiXmpY7qHUM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hjvRVM6QEVf"
      },
      "outputs": [],
      "source": [
        "#lets check out a few predictions and their different values\n",
        "i_multiplier = 0\n",
        "num_rows = 3\n",
        "num_cols = 2\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(10*num_cols, 5*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_pred(prediction_probabilities=predictions,\n",
        "            labels=val_labels,\n",
        "            images=val_images,\n",
        "            n=i+i_multiplier)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_pred_conf(prediction_probabilities=predictions,\n",
        "                 labels=val_labels,\n",
        "                 n=i+i_multiplier)\n",
        "plt.tight_layout(h_pad=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Saving and reloading a trained model"
      ],
      "metadata": {
        "id": "HPLSdrGBu0SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to save a model\n",
        "def save_model(model, suffix=None):\n",
        "  \"\"\"\n",
        "  saves a given model in a models directory and appends a suffix (string).\n",
        "  \"\"\"\n",
        "\n",
        "  #Create a model directory pathname with current time\n",
        "  modeldir = os.path.join(\"drive/MyDrive/dog-vision/dog-breed-identification/models\",\n",
        "                       datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  model_path = modeldir + \"-\" + suffix + \".h5\"\n",
        "  print(f\"Saving model to: {model_path}...\")\n",
        "  model.save(model_path)\n",
        "  return model_path"
      ],
      "metadata": {
        "id": "9ZzrUwohrp3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a fuction to load aa trained model\n",
        "def load_model(model_path):\n",
        "  \"\"\"\n",
        "  Load a saved model from a specified path.\n",
        "  \"\"\"\n",
        "\n",
        "  print(f\"Loading saved model from: {model_path}\")\n",
        "  model = tf.keras.models.load_model(model_path,\n",
        "                                     custom_objects={\"kerasLayer\":hub.kerasLayer})\n",
        "  return model"
      ],
      "metadata": {
        "id": "6rfMVqzmvYls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got function to save and load a trined model, let's make sure they work!\n"
      ],
      "metadata": {
        "id": "iPjpE1Hx6csb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save our model trained on 1000 images\n",
        "save_model(model, suffix=\"1000-images-mobilenetv2-adam\")"
      ],
      "metadata": {
        "id": "KtZaeXOI6bzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lZchdju1CwKO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
